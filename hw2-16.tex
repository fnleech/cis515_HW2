\documentclass[12pt]{article}
\usepackage{amsfonts,amssymb,amsmath}
%\documentstyle[12pt,amsfonts]{article}
%\documentstyle{article}

\setlength{\topmargin}{-.5in}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\textwidth}{6.5truein}
\setlength{\textheight}{8.5truein}
%\input ../basicmath/basicmathmac.tex
%
%\input ../lamacb.tex
\input ../cis515_HW2/mac.tex
\input ../cis515_HW2/mathmac.tex

\def\fseq#1#2{(#1_{#2})_{#2\geq 1}}
\def\fsseq#1#2#3{(#1_{#3(#2)})_{#2\geq 1}}
\def\qleq{\sqsubseteq}

%
\begin{document}
\begin{center}
\fbox{{\Large\bf Fall 2016 \hspace*{0.4cm} CIS 515}}\\
\vspace{1cm}
{\Large\bf Fundamentals of Linear Algebra and Optimization\\
Jean Gallier \\
\vspace{0.5cm}
Homework 2}\\[10pt]
September, 20 2016; Due October 11, 2016\\
Francine Leech, Chen Xiang, Reffat Manzur
\end{center}


\vspace {0.25cm}\noindent
{\bf Problem B1 (10 pts).} \\
Suppose $A = (a_{i,j})_{m \times n}$, $B = (b_{i,j})_{n \times p}$, C = $AB = (c_{i,j})_{m \times p}$.
It is easy to write $c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj},\; \forall i = 1,2,\cdots m.\; j = 1,2,\cdots p$ 
and then consider $(A^1B_1 + \cdots + A^nB_n)_{ij}$, we have $  (A^1B_1 + \cdots + A^nB_n)_{ij} = (a_{i1}b_{1j} + a_{i2}b_{2j} \cdots a_{in}b_{nj}) = \sum_{k=1}^{n} a_{ik} b_{kj}$. So $AB = A^1B_1 + \cdots + A^nB_n$.

\medskip
\vspace {0.25cm}\noindent
{\bf Problem B2 (10 pts).} \\
Because $f$ is a linear map, thus we have 
\[
f(x + y) = f(x) + f(y)
\]
\[
f(\lambda x) = \lambda f(x)
\]
So write the inverse function $g = f^{-1}$ and $x_1,x_2 \in E$, we have 
\begin{align*}
\exists x \in E , \; x = g(f(x_1) + f(x_2)) &\Rightarrow 
f(x) = f(x_1) + f(x_2) = f(x_1 + x_2) \\
&\Rightarrow x = x_1 + x_2 \\
&\Rightarrow g(f(x)) = g(f(x_1 + x_2)) = g(f(x_1)) + g(f(x_2)) \\
\exists x^* \in E, \; x^* = g(f(\lambda x)) &\Rightarrow f(x^*) = f(\lambda x) \\
&\Rightarrow x^* = \lambda x \\
&\Rightarrow g(\lambda f(x)) = g(f(\lambda x)) = \lambda g(f(x))
\end{align*}



\vspace {0.25cm}\noindent
{\bf Problem B3 (10 pts).} \\
For $x\epsilon E$, it can be written as $x_{1}u_{1}+x_{2}u_{2}+\cdots+x_{n}u_{n}$as
$(u_{i})_{i\epsilon I}$is a basis of $E$ and $(u_{i})_{i\epsilon I}$spans
$E$ . Then, $f(x)=x_{1}f(u_{1})+x_{2}f(u_{2})+\cdots+x_{n}f(u_{n})$.
Since $f(u_{i})=v_{i}$, this can be rewritten as: $f(x)=x_{1}v_{1}+x_{2}v_{2}+\cdots+x_{n}v_{n}$.
If the linear map $f$ is surjective, then every element in $F$ has
a corresponding element in $E$ and $f(x)=x_{1}v_{1}+x_{2}v_{2}+\cdots+x_{n}v_{n}$
indicates that $(v_{i})_{i\epsilon I}$ must span $F$.\\

\vspace {0.25cm}\noindent
{\bf Problem B4 (10 pts).} \\
Let $\mapdef{f}{E}{F}$ be a linear map 
with $\mathrm{dim}(E) = n$ and $\mathrm{dim}(F)  = m$.
Prove that $f$ has rank $1$ iff $f$ is represented by an
$m\times n$ matrix of the form
\[
A = u\transpos{v}
\]
with $u$ a nonzero column vector of dimension $m$ and $v$ a 
nonzero column vector  of dimension $n$.
$dim(E)=n$ indicates that the basis of $E$ consists of $n$ vectors
and $dim(F)=m$ indicates the basis of $F$ consists of $m$ vectors.
From class notes, we know that $M(f)=\begin{pmatrix}a_{11} & \cdots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{m1} & \cdots & a_{mn}
\end{pmatrix}=A$, an $mxn$ matrix.

$u=\begin{pmatrix}u_{1}\\
u_{2}\\
u_{3}\\
\vdots\\
u_{m}
\end{pmatrix}$ and $v^{T}=\begin{pmatrix}v_{1} & v_{2} & v_{3} & \cdots & v_{n}\end{pmatrix}$
so $uv^{T}$ is an $mxn$ matrix of the form: $\begin{pmatrix}u_{1}v_{1} & u_{1}v_{2} & u_{1}v_{3} & \cdots & u_{1}v_{n}\\
u_{2}v_{1} & u_{2}v_{2} & u_{2}v_{3} & \cdots & u_{2}v_{n}\\
u_{3}v_{1} & u_{3}v_{2} & u_{3}v_{3} & \cdots & u_{3}v_{n}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
u_{m}v_{1} & u_{m}v_{2} & u_{m}v_{3} & \cdots & u_{m}v_{n}
\end{pmatrix}=v_{1}\begin{pmatrix}u_{1}\\
u_{2}\\
u_{3}\\
\vdots\\
u_{m}
\end{pmatrix}+v_{2}\begin{pmatrix}u_{1}\\
u_{2}\\
u_{3}\\
\vdots\\
u_{m}
\end{pmatrix}+\cdots v_{n}\begin{pmatrix}u_{1}\\
u_{2}\\
u_{3}\\
\vdots\\
u_{m}
\end{pmatrix}$. Since every column in the matrix of $f$ is a multiple of one column,
the rank is 1 for $f$ and the matrix is always of this form by definition
for an $mxn$ matrix.



\medskip
\vspace {0.25cm}\noindent
{\bf Problem B5 (120 pts).} \\
\medskip
(1)

\[
W_{3, 3} c =
\begin{pmatrix}
1  &  0  &  0 & 0 & 1   &  0   & 0    & 0  \\
1  &  0  &  0 & 0 & -1 &  0   & 0    & 0  \\
0   &  1  &  0 & 0 &  0  & 1   & 0    & 0  \\
0   &  1  &  0 & 0 &  0  & -1 & 0    & 0  \\
0   &  0  &  1 & 0 &  0  &  0  & 1    & 0  \\
0   &  0  &  1 & 0 &  0  &  0  & -1  & 0  \\
0   &  0  &  0 & 1 &  0  &  0  & 0    & 1  \\
0   &  0  &  0 & 1 &  0  &  0  & 0    & -1  \\
\end{pmatrix}
\cdot
\begin{pmatrix}
c_1 & \\
c_2 & \\
c_3 & \\
c_4 & \\
c_5 & \\
c_6 & \\
c_7 & \\
c_8 \\
\end{pmatrix}
\]

\[
= 
\begin{pmatrix}
1 \cdot c_1 + 1 \cdot c_5 \\
1 \cdot c_1 + -1 \cdot c_5 \\
1 \cdot c_2 + 1 \cdot c_6 \\
1 \cdot c_2 + -1 \cdot c_6\\
1 \cdot c_3 + 1 \cdot c_7 \\
1 \cdot c_3 + - 1 \cdot c_7 \\
1 \cdot c_4 + 1 \cdot c_8 \\
1 \cdot c_4 + -1 \cdot c_8 \\
\end{pmatrix}
\\ 
= 
\begin{pmatrix}
c_1 + c_5 \\
c_1 - c_5 \\
c_2 + c_6 \\
c_2 - c_6\\
c_3 + c_7 \\
c_3 - c_7 \\
c_4 + c_8 \\
c_4 - c_8 \\
\end{pmatrix}
\]

\medskip
(2) \\
If the inverse of $W_{3, 3}$ is $(1/2) \transpos{W_{3, 3}}$, then the product of the two matrices is the identity matrix. 
\[
W_{3, 3} \cdot (1/2) \transpos{W_{3, 3}}=
\begin{pmatrix}
1  &  0  &  0 & 0 & 1   &  0   & 0    & 0  \\
1  &  0  &  0 & 0 & -1 &  0   & 0    & 0  \\
0   &  1  &  0 & 0 &  0  & 1   & 0    & 0  \\
0   &  1  &  0 & 0 &  0  & -1 & 0    & 0  \\
0   &  0  &  1 & 0 &  0  &  0  & 1    & 0  \\
0   &  0  &  1 & 0 &  0  &  0  & -1  & 0  \\
0   &  0  &  0 & 1 &  0  &  0  & 0    & 1  \\
0   &  0  &  0 & 1 &  0  &  0  & 0    & -1  \\
\end{pmatrix}
\cdot
\begin{pmatrix}
0.5 & 0.5  & 0   & 0    & 0    & 0     & 0    & 0    \\ 
0   & 0    & 0.5 & 0.5  & 0    & 0     & 0    & 0    \\ 
0   & 0    & 0   & 0    & 0.5  & 0.5   & 0    & 0     \\
0   & 0    & 0   & 0    & 0    & 0     & 0.5  & 0.5   \\
0.5 & -0.5 & 0   & 0    & 0    & 0     & 0    & 0     \\
0   & 0    & 0.5 & -0.5 & 0    & 0     & 0    & 0     \\
0   & 0    & 0   & 0    & 0.5  & -0.5  & 0    & 0     \\
0   & 0    & 0   & 0    & 0    & 0     & 0.5  & -0.5  \\
\end{pmatrix}
\] 

\[
= 
\begin{pmatrix}
0.5 + 0.5  & 0.5 + -0.5 & 0          & 0          & 0          & 0          & 0          & 0          \\
0.5 + -0.5 & 0.5 --0.5  & 0          & 0          & 0          & 0          & 0          & 0          \\
0          & 0          & 0.5 + 0.5  & 0.5 +-0.5  & 0          & 0          &  0          & 0          \\
0          & 0          & 0.5 +-0.5  & 0.5 --0.5  & 0          & 0          & 0          & 0          \\
0          & 0          & 0          & 0          & 0.5 + 0.5  & 0.5 +-0.5  & 0          & 0          \\
0          & 0          & 0          & 0          & 0.5 +-0.5  & 0.5 --0.5  & 0          & 0          \\
0          & 0          & 0          & 0          & 0          & 0          & 0.5 + 0.5  & 0.5 +-0.5  \\
0          & 0          & 0          & 0          & 0          & 0             & 0.5 +-0.5  &  0.5 --0.5\\ 
\end{pmatrix}
\] 

\[
=
\begin{pmatrix}
1  & 0 & 0          & 0          & 0          & 0          & 0          & 0          \\
0 & 1  & 0          & 0          & 0          & 0          & 0          & 0          \\
0          & 0          & 1  & 0  & 0          & 0          &  0          & 0          \\
0          & 0          & 0  & 1 & 0          & 0          & 0          & 0          \\
0          & 0          & 0          & 0          & 1  & 0  & 0          & 0          \\
0          & 0          & 0          & 0          & 0  & 1  & 0          & 0          \\
0          & 0          & 0          & 0          & 0          & 0          & 1  & 0  \\
0          & 0          & 0          & 0          & 0          & 0             & 0  &  1\\ 
\end{pmatrix}
\] 

\medskip
(3) \\%
\pmb{First: $W_{3, 2} c = (c_1 + c_3, c_1- c_3, c_2 + c_4, c_2 - c_4, c_5, c_6, c_7, c_8)$}

\[
W_{3, 2} c =
\begin{pmatrix}
1  &  0  & 1    &  0   & 0    & 0  &  0  &  0  \\
1  &  0  &  -1 &  0   & 0    & 0  &  0  &  0 \\
0   &  1  &  0  &  1   & 0    & 0  &  0  &  0 \\
0   &  1  &  0  &  -1 & 0    & 0  &  0  &  0 \\
0   &  0  &  0 & 0 &  1  &  0  & 0    & 0  \\
0   &  0  &  0 & 0 &  0  &  1  &  0   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  &  1   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 0    & 1  \\
\end{pmatrix}
\cdot 
\begin{pmatrix}
c_1 \\
c_2 \\
c_3 \\
c_4 \\
c_5 \\
c_6 \\
c_7 \\
c_8 \\
\end{pmatrix}
=
\begin{pmatrix}
c_1 + 0 + c_3 + 0 + 0 + 0 + 0 +0 \\
c_1 + 0 - c_3 + 0 + 0 + 0 + 0 +0 \\
0 + c_2 + 0 + c_4 + 0 + 0 + 0 +0 \\
0 + c_2 + 0 - c_4 + 0 + 0 + 0 +0 \\
0 + 0 + 0 + 0 + c_5 + 0 + 0 + 0  \\
0 + 0 + 0 + 0 + 0 + c_6 + 0 + 0 \\
0 + 0 + 0 + 0 + 0 + 0 + c_7 + 0 \\
0 + 0 + 0 + 0 + 0 + 0 + 0 + c_8 \\
\end{pmatrix}
=
\begin{pmatrix}
c_1 + c_3 \\
c_1 - c_3 \\
c_2 + c_4 \\
c_2 - c_4 \\
c_5 \\
c_6 \\
c_7 \\
c_8 \\
\end{pmatrix}
\]
\pmb{Second: $W_{3, 1} c = (c_1 + c_2, c_1- c_2, c_3,  c_4, c_5, c_6, c_7, c_8)$} 

\[
W_{3, 1} c =
\begin{pmatrix}
1  &  1   & 0    &  0   & 0    & 0  &  0  &  0  \\
1  & -1  &  0   &  0   & 0    & 0  &  0  &  0 \\
0   &  0  &  1   &   0   & 0    & 0  &  0  &  0 \\
0   &  0   &  0  &   1 & 0    & 0  &  0  &  0 \\
0   &  0  &  0 & 0 &  1  &  0  & 0    & 0  \\
0   &  0  &  0 & 0 &  0  &  1  &  0   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  &  1   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 0    & 1  \\
\end{pmatrix}
\cdot 
\begin{pmatrix}
c_1 \\
c_2 \\
c_3 \\
c_4 \\
c_5 \\
c_6 \\
c_7 \\
c_8 \\
\end{pmatrix}
=
\begin{pmatrix}
c_1 + c_2 + 0 + 0 + 0 + 0 + 0 +0 \\
c_1 - c_2 + 0 + 0 + 0 + 0 + 0 +0 \\
0 + 0+ c_3 + 0 + 0 + 0 + 0 +0 \\
0 + 0 + 0 + c_4 + 0 + 0 + 0 +0 \\
0 + 0 + 0 + 0 + c_5 + 0 + 0 + 0  \\
0 + 0 + 0 + 0 + 0 + c_6 + 0 + 0 \\
0 + 0 + 0 + 0 + 0 + 0 + c_7 + 0 \\
0 + 0 + 0 + 0 + 0 + 0 + 0 + c_8 \\
\end{pmatrix}
=
\begin{pmatrix}
c_1 + c_2 \\
c_1 - c_2 \\
c_3 \\
c_4 \\
c_5 \\
c_6 \\
c_7 \\
c_8 \\
\end{pmatrix}
\]
\\
\pmb{Now check $W_{3, 2}*W_{3, 1}$}
\\
\[
W_{3, 2}*W_{3, 1} = 
\begin{pmatrix}
1  &  0  & 1    &  0   & 0    & 0  &  0  &  0  \\
1  &  0  &  -1 &  0   & 0    & 0  &  0  &  0 \\
0   &  1  &  0  &  1   & 0    & 0  &  0  &  0 \\
0   &  1  &  0  &  -1 & 0    & 0  &  0  &  0 \\
0   &  0  &  0 & 0 &  1  &  0  & 0    & 0  \\
0   &  0  &  0 & 0 &  0  &  1  &  0   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  &  1   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 0    & 1  \\
\end{pmatrix}
\begin{pmatrix}
1  &  1   & 0    &  0   & 0    & 0  &  0  &  0  \\
1  & -1  &  0   &  0   & 0    & 0  &  0  &  0 \\
0   &  0  &  1   &   0   & 0    & 0  &  0  &  0 \\
0   &  0   &  0  &   1 & 0    & 0  &  0  &  0 \\
0   &  0  &  0 & 0 &  1  &  0  & 0    & 0  \\
0   &  0  &  0 & 0 &  0  &  1  &  0   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  &  1   & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 0    & 1  \\
\end{pmatrix}
\]
\[
=
\begin{pmatrix}
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\   
1 & 1 & -1 & 0 & 0 & 0 & 0 & 0 \\
1 & -1 & 0 & 1 & 0 &  0 &  0 &  0 \\
1 & -1 & 0 & -1 & 0 & 0 & 0 &  0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{pmatrix}
= 
\begin{pmatrix}
W_2 & 0_{4, 4} \\
0_{4, 4}         & I_4
\end{pmatrix},
\]
\\
\pmb{Conclude $W_{3, 3}*W_{3, 2}*W_{3, 1} = W_3$}
\\
\[
W_{3, 3}*W_{3, 2}*W_{3, 1} = 
\begin{pmatrix}
1  &  0  &  0 & 0 & 1   &  0   & 0    & 0  \\
1  &  0  &  0 & 0 & -1 &  0   & 0    & 0  \\
0   &  1  &  0 & 0 &  0  & 1   & 0    & 0  \\
0   &  1  &  0 & 0 &  0  & -1 & 0    & 0  \\
0   &  0  &  1 & 0 &  0  &  0  & 1    & 0  \\
0   &  0  &  1 & 0 &  0  &  0  & -1  & 0  \\
0   &  0  &  0 & 1 &  0  &  0  & 0    & 1  \\
0   &  0  &  0 & 1 &  0  &  0  & 0    & -1  \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\   
1 & 1 & -1 & 0 & 0 & 0 & 0 & 0 \\
1 & -1 & 0 & 1 & 0 &  0 &  0 &  0 \\
1 & -1 & 0 & -1 & 0 & 0 & 0 &  0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{pmatrix}
\]
\[
= 
\begin{pmatrix}
1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\   
1 & 1 & 1 & 0 & -1 & 0 & 0 & 0 \\
1 & 1 & -1 & 0 & 0 &  1 &  0 &  0 \\
1 & 1 & -1 & 0 & 0 & -1 & 0 &  0 \\
1 & -1 & 0 & 1 & 0 & 0 & 1 & 0 \\
1 & -1 & 0 & 1 & 0 & 0 & -1 & 0 \\
1 & -1 & 0 & -1 & 0 & 0 & 0 & 1 \\
1 & -1 & 0 & -1 & 0 & 0 & 0 & -1 \\
\end{pmatrix}
\]


We see that in the first row of the $W_{3,3}$ matrix, there is at least $1$ $1$ in the columns $1$ through $4$ in each of the $8$ rows. We are guaranteed that the first column in the $W_3$ matrix is all $1$'s because, every $1$ in $W_{3,3}$ will be multiplied by at least $1$'s located in the first $4$ rows in the first column of  $W_{3,2}*W_{3,1}$. We can apply the same theory to get the second column of $W_3$. The change here is that there are $2$ $-1$'s located in the 3rd and 4th row in the second column of $W_{3,2}*W_{3,1}$. Now, the last 4 rows in the second column of $W_3$ will be $-1$. Column 3 in $W_3$ will have a  1 in the first 2 rows and a -1 in the next two rows because only the first 4 rows in $W_{3,3}$ have $1$'s in the first two columns. Using the same idea, rows 5 and 6 will have 1 and the last two rows will have -1 in $W_3$. Finally, the last 4 columns in $W_3$ will have a 1 and -1. The 1 and -1 in $W_{3,3}$ is in the 5th, 6th, 7th, and 8th columns, and the last 4 columns in $W_{3,2}*W_{3,1}$ are $0s$ in the first $4$ rows and an $4x4$ identity matrix in the last $4$ rows. With the row and column multiplication, we are guaranteed the shifting 1 and -1 in the last 4 columns of $W_3$. 

\medskip
(4) 

We have $W_{3, 3} = (\alpha_1, \alpha_2, \cdots, \alpha_8)$, then write $\alpha_i^T\alpha_j, \forall i,j = 1,2,\cdots8 \; i \neq j$ there are two conditions: \\
1. $\alpha_i$ and $\alpha_j$ have elements on the same positions. \\ 
2. $\alpha_i$ and $\alpha_j$ have elements on different postions. \\
For the first condition, we have $\alpha_i^T\alpha_j = 1-1=0$ and for the second condition, we have $\alpha_i^T\alpha_j = 0$. Also, $\alpha_i^T\alpha_i = 1 + 1 = 2$, so we can prove that the columns are orthogonal. It
is the same to prove the rows are orthogonal. \\
\\
\pmb{Inverse of $W_{3, 2}$}
\\
We see that in $W_{3, 2}$ , the last 4 rows already have a diagonal with all $1$'s. The only thing left is to manipulate the first 4 rows. Similar to $W_{3,3}$, the transpose of $W_{3, 2}$ is $(1/2) \transpos{W_{3, 2}}$ of the first 4 rows and 4 columns.\\
\[
inv(W_{3, 2}) = 
\begin{pmatrix}
0.5  &  0.5  &  0 & 0 & 0   &  0   & 0    & 0  \\
0  &  0  &  0.5 & 0.5 & 0 &  0   & 0    & 0  \\
0.5   &  -0.5  &  0 & 0 &  0  & 0   & 0    & 0  \\
0   &  0  &  0.5 & -0.5 &  0  & 0 & 0    & 0  \\
0   &  0  &  0 & 0 &  1  &  0 & 0    & 0  \\
0   &  0  &  0 & 0 &  0  &  1 & 0  & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 1    & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 0    & 1 \\
\end{pmatrix}
\]
\\
\pmb{Inverse of $W_{3, 1}$ }
\\
6 rows of $W_{3, 1}$ is already the identity matrix. Thus only the first $2x2$ submatrix in $W_{3, 1}$ has to be manipulated to find it's inverse. We can use a similar method as above, to find that we only work that has to be done is taking the $\frac{1}{2} \transpos{W_{3, 1}}$ of the $2x2$ submatrix. \\
\[
inv(W_{3, 1}) = 
\begin{pmatrix}
0.5  &  0.5  &  0 & 0 & 0   &  0   & 0    & 0  \\
0.5 &  -0.5  &  0 & 0 & 0 &  0   & 0    & 0  \\
0   &  0  &  1 & 0 &  0  & 0   & 0    & 0  \\
0   &  0  &  0 & 1 &  0  & 0 & 0    & 0  \\
0   &  0  &  0 & 0 &  1  &  0 & 0    & 0  \\
0   &  0  &  0 & 0 &  0  &  1 & 0  & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 1    & 0  \\
0   &  0  &  0 & 0 &  0  &  0  & 0    & 1 \\
\end{pmatrix}
\]
\\

\medskip
(5) We can find that 
\begin{align*}
W_{n,n} = 
\begin{pmatrix}
1 &\overbrace{0 \cdots 0}^{2^{n-1}-1} &1 &\overbrace{0 \cdots 0}^{2^{n-1}-1} \\
1 &\overbrace{0 \cdots 0}^{2^{n-1}-1} &-1 &\overbrace{0 \cdots 0}^{2^{n-1}-1} \\
0 &1 &\overbrace{\cdots 0}^{2^{n-1}-1} &1 &\overbrace{0 \cdots 0}^{2^{n-1}-1} \\
0 &1 &\overbrace{\cdots 0}^{2^{n-1}-1} &-1 &\overbrace{0 \cdots 0}^{2^{n-1}-1} \\
\cdots &\cdots &\cdots &\cdots &\cdots \\
&\overbrace{\cdots 0}^{2^{n-1}-1} &1 &\overbrace{\cdots 0}^{2^{n-1}-1} &1 \\
&\overbrace{\cdots 0}^{2^{n-1}-1} &1 &\overbrace{\cdots 0}^{2^{n-1}-1} &-1
\end{pmatrix}
\end{align*}
So 
\begin{align*}
W_{n,n}c =
\begin{pmatrix}
c_1 + c_{2^{n-1} + 1} \\
c_1 - c_{2^{n-1} + 1} \\
c_2 + c_{2^{n-1} + 2} \\
c_2 - c_{2^{n-1} + 2} \\
c_3 + c_{2^{n-1} + 3} \\
c_3 - c_{2^{n-1} + 3} \\
 \vdots \\
c_{2^{n-1}} + c_{2^{n}} \\
c_{2^{n-1}} - c_{2^{n}}
\end{pmatrix}
\end{align*}
It is the last step in process of reconstructing a vector from its Haar coefficients c. \\
Write $W_{n,n} = 
\begin{pmatrix}
v_1^T \\
v_2^T \\
\vdots \\
v_{2^{n}}^T 
\end{pmatrix}
$, So $W_{n,n}^T W_{n,n} = 
\begin{pmatrix}
v_1 &v_2 &v_3 \cdots &v_{2^n}
\end{pmatrix}
\begin{pmatrix}
v_1^T \\
v_2^T \\
\vdots \\
v_{2^{n}}^T 
\end{pmatrix}　=　
\sum_{i=1}^{2^n} v_iv_i^T
$.
Because we can calculate that 
$
v_i v_i^T = 
\begin{pmatrix}
0 &0 &0 &\cdots &0 \\
\vdots &\vdots &\vdots &\vdots &\vdots\\
0 &\cdots &1 &\cdots &0 \\
\vdots &\vdots &\vdots &\vdots &\vdots \\
0 &\cdots &0 &1 &\cdots \\
\vdots &\vdots &\vdots &\vdots &\vdots \\
0 &0 &0 &\cdots &0 
\end{pmatrix}
$ there are two $1$ in the diagonal. Because we have pairs 
$u_i = (\underbrace{0,0,\cdots, 1, \cdots,0,0,}_{2^{n-1}}\underbrace{0,0,\cdots, 1, \cdots,0,0}_{2^{n-1}})$ and $v_i = (\underbrace{0,0,\cdots, 1, \cdots,0,0,}_{2^{n-1}}\underbrace{0,0,\cdots, -1, \cdots,0,0}_{2^{n-1}})$, also there is $uu^T + vv^T = 
\begin{pmatrix}
0 &0 &0 &\cdots &0 \\
\vdots &\vdots &\vdots &\vdots &\vdots\\
0 &\cdots &2 &\cdots &0 \\
\vdots &\vdots &\vdots &\vdots &\vdots \\
0 &\cdots &0 &2 &\cdots \\
\vdots &\vdots &\vdots &\vdots &\vdots \\
0 &0 &0 &\cdots &0 
\end{pmatrix}
$. When $i \neq j, \; (u_iu_i^T)\cdot(u_ju_j^T) = \mathbf{0}, \;
(v_iv_i^T)\cdot(v_jv_j^T) = \mathbf{0}
 $, thus 
 $
 W_{n,n}^T W_{n,n} = 2I 
 \Rightarrow 
 W_{n,n}^{-1} = \frac{1}{2} W_{n,n}^T 
 $.
 When $W_{n,n} = 
\begin{pmatrix}
v_1^T \\
v_2^T \\
\vdots \\
v_{2^{n}}^T 
\end{pmatrix}
$, it is easy to calculate $
v_i^T \cdot v_i = 2, \;
$, $v_i^T \cdot v_j = 0$ or $v_i^T \cdot v_j = 1-1 = 0, \; i \neq j$. So rows are orthogonal, it is the same to prove columns are orthogonal. 


\medskip
{\bf Extra credit (30 pts.)}

We can find \[
\begin{pmatrix}
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
& 
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\end{pmatrix} \in M_{2^n,2^n}
\] 
and it is easy to get 
\[
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & \cdots & 0 \\
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & \vdots \\
0 & 1 & \cdots & \vdots \\
\vdots  & \vdots & \ddots & \vdots \\
0 & 0 & \cdots &1 \\
0 & 0 & \cdots &1 \\
\end{pmatrix} 
\in M_{2^n, 2^{n-1}},
\]
\[
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & \cdots & 0 \\
-1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & \vdots \\
0 & -1 & \cdots & \vdots \\
\vdots  & \vdots & \ddots & \vdots \\
0 & 0 & \cdots &1 \\
0 & 0 & \cdots &-1 \\
\end{pmatrix} 
\in M_{2^n, 2^{n-1}},
\]
so 
\[
\begin{pmatrix}
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
&
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & \cdots & \cdots  & 1  & 0 & 0 \\
1 & 0 & \cdots & \cdots  & -1 & 0 & 0 \\
0 & 1 & \cdots & \cdots  & 0  & 1 & \vdots \\
0 & 1 & \cdots & \cdots  & 0  & -1 & \vdots \\
\vdots  & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots &1 &0  & \cdots & 1 \\
0 & 0 & \cdots &1 &0  & \cdots & -1 
\end{pmatrix} 
= W_{n,n}.
\]
\begin{align*}
W_{n, n} \transpos{W_{n, n}} &= 
\begin{pmatrix}
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
& 
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\end{pmatrix}
\transpos{
\begin{pmatrix}
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
& 
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\end{pmatrix}} \\
&=
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
\transpos{[
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}
]} + 
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}
\transpos{[
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}]
} \\
\end{align*}
according to the facts above, we have
\begin{align*}
W_{n, n} \transpos{W_{n, n}} &= 
[I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}]
[I_{2^{n-1}} \tensor 
\transpos{\begin{pmatrix}
1 \\
1
\end{pmatrix}
}] + 
[I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}]
[I_{2^{n-1}} \tensor 
\transpos{
\begin{pmatrix}
1 \\
-1
\end{pmatrix}}] \\
&=
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 &1 \\
1 &1 
\end{pmatrix} +
I_{2^{n-1}} \tensor 
\begin{pmatrix}
1 &-1 \\
-1 &1 
\end{pmatrix} \\
&= 
I_{2^{n-1}} \tensor 
\begin{pmatrix}
2 &0 \\
0 &2 
\end{pmatrix} = 
2I_{2^n} 
\end{align*}
Use Induction: \\
When $n = 1$ we have $W_1 = 
\begin{pmatrix}
1 &1 \\
1 &-1
\end{pmatrix}$, So $
W_1^T \cdot W_1 = 
\begin{pmatrix}
2 &0 \\
0 &2
\end{pmatrix} = B_1
$. \\
Suppose it is true when $n = k$, we have $W_k^T W_k = B_k$, then when $n = k + 1$ 
\begin{align*}
W_{k+1}^TW_{k+1} &= 
\begin{pmatrix}
[W_k \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}]^T \\
[I_{2^k} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}]^T 
\end{pmatrix} \cdot
\begin{pmatrix}
[W_k \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}]
[I_{2^k} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}]
\end{pmatrix} \\
&=
\begin{pmatrix}
[W_k^T \tensor 
\begin{pmatrix}
1 
&1
\end{pmatrix}]
[W_k \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}]
&
[W_k^T \tensor 
\begin{pmatrix}
1 
&1
\end{pmatrix}]
[I_{2^k} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}]
\\
[I_{2^k} \tensor 
\begin{pmatrix}
1 
&-1
\end{pmatrix}]
[W_k \tensor 
\begin{pmatrix}
1 \\
1
\end{pmatrix}]
&
[I_{2^k} \tensor 
\begin{pmatrix}
1 
&-1
\end{pmatrix}]
[I_{2^k} \tensor 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}]
\end{pmatrix} \\
&= 
\begin{pmatrix}
W_k^TW_k \tensor I  & \mathbf{0} \\
\mathbf{0} & I_{2^k} \tensor 2
\end{pmatrix} \\
&=
\begin{pmatrix}
2B_k \tensor I  & \mathbf{0} \\
\mathbf{0} & 2I_{2^k} 
\end{pmatrix} =
2\begin{pmatrix}
B_k & \mathbf{0} \\
\mathbf{0} &I_{2^k} 
\end{pmatrix} \\
&=
B_{k+1}　
\end{align*}
So $W_n^TW_n = B_n$.




\medskip
(6)
The matrix $W_{n, i}$ is obtained from the matrix $W_{i, i}$ 
($1 \leq i \leq n - 1$) as follows:
\[
W_{n, i} = 
\begin{pmatrix}
 W_{i, i} & 0_{2^{i}, 2^{n} - 2^{i}} \\
0_{2^{n}- 2^{i}, 2^i} & I_{2^{n} - 2^{i}}
\end{pmatrix}.
\]
It consists of four blocks,
where  $0_{2^{i}, 2^{n} - 2^{i}}$ and $0_{2^{n} - 2^{i}, 2^i}$
are matrices of zeros and $I_{2^{n} - 2^{i}}$ is the identity matrix of
dimension $2^{n} - 2^i$.

\medskip
Explain what $W_{n, i}$ does to $c$ and prove that
\[
W_{n, n} W_{n, n - 1} \cdots W_{n, 1} = W_n,
\]
where $W_n$ is the Haar matrix of dimension $2^n$.

\medskip
\hint
Use induction on $k$,  with the induction hypothesis
\[
W_{n, k} W_{n, k - 1}\cdots W_{n, 1} = 
\begin{pmatrix}
 W_{k} & 0_{2^{k}, 2^{n} - 2^{k}} \\
0_{2^{n}- 2^{k}, 2^k} &  I_{2^{n} - 2^{k}}
\end{pmatrix}.
\]


\medskip
Prove that the columns and rows of $W_{n, k}$ are orthogonal, and use
this to prove that the columns of $W_n$ and the rows of
$W_n^{-1}$ are orthogonal. 
Are the rows of  $W_n$ orthogonal?
Are the columns of  $W_n^{-1}$ orthogonal?
Prove that
\[
W_{n, k}^{-1} = 
\begin{pmatrix}
 \frac{1}{2}\transpos{W_{k, k}} & 0_{2^{k}, 2^{n} - 2^{k}} \\
0_{2^{n}- 2^{k}, 2^k} &  I_{2^{n} - 2^{k}}
\end{pmatrix}.
\]


\vspace {0.25cm}\noindent
{\bf Problem B6 (20 pts).}
We can use Proposition 3.1 to prove that $E=Ker(f)\oplus Im(f)$.
According to the proposition, given vector space $E$, $Ker(f)+Im(f)$
is a direct sum iff $Ker(f)\cap Im(f)=(0)$. \\
Let $u \in E$ then calculate $v = u - f(u)$, because $f$ is an idempotent linear map $f \circ f = f$ which means $f(v) = f(u - f(u)) = f(u) - f^2(u) = 0$. Thus $v \in ker(f)$, So $E = Ker(f) + Im(f)$. Let $u^* \in Ker(f) \cap Im(f)$, then $u^* = f(s), \exists s \in E$.Because $s = f(u^*) = f \circ f(u^*) = 0$ and $f$ is a linear map, $u^* = f(s) = 0$, which means $Ker(f) \cap Im(f)=(0)$, then $E = Ker(f) \oplus Im(f)$

\medskip
\vspace {0.25cm}\noindent
{\bf Problem B7 (20 pts).}
Let $U_1, \ldots, U_p$ be any  $p \geq 2$  subspaces of some vector
space $E$ and recall that the linear map
\[
\mapdef{a}{U_1\times \cdots \times U_p}{E}
\]
is given by
\[
a(u_1, \ldots, u_p) = u_1 + \cdots + u_p,
\]
with $u_i \in U_i$ for $i = 1, \ldots, p$.

\medskip
(1)
If we let $Z_i \subseteq U_1\times \cdots \times U_p$ be given by
\[
Z_i = \left.\bigg\{\Big(u_1, \ldots, u_{i - 1}, -\sum_{j = 1, j \not= i}^p u_j, u_{i + 1}, 
\ldots, u_p\Big) \>\right|\> \sum_{j = 1, j \not= i}^p u_j  \in  
U_i \cap \bigg(\sum_{j = 1, j \not= i}^p U_j \bigg) 
\bigg\},
\]
for $i = 1, \ldots, p$, then prove that
\[
\Ker{a} = Z_1 = \cdots =   Z_p.
\]
In general, for any given $i$, the condition
$U_i \cap \bigg(\sum_{j = 1, j \not= i}^p U_j \bigg) =
(0)$ does not necessarily imply that $Z_i = (0)$.
Thus, let 
\[
Z = \left.\bigg\{\Big(u_1, \ldots, u_{i - 1}, u_i, u_{i + 1}, 
\ldots, u_p\Big) \>\right|\> u_i =  -\sum_{j = 1, j \not= i}^p u_j, \>
u_i   \in  
U_i \cap \bigg(\sum_{j = 1, j \not= i}^p U_j \bigg), \> 1\leq i \leq p 
\bigg\}.
\]
Since $\Ker{a} = Z_1 = \cdots =  Z_p$, we have $Z = \Ker a$.
Prove that if
\[
U_i \cap \bigg(\sum_{j = 1, j \not= i}^p U_j \bigg) = (0)
\quad 1 \leq i \leq p,
\]
then $Z = \Ker a = (0)$.

\medskip
(2)
Prove that $U_1 + \cdots + U_p$ is a direct sum iff
\[
U_i \cap \bigg(\sum_{j = 1, j \not= i}^p U_j \bigg) = (0)
\quad 1 \leq i \leq p.
\] \\

We can use the definition of a vector space to prove this. The vector space $E$ is the direct sum of 2 more subspaces, $U_1 + \cdots + U_p$. The subspaces span the whole space $E$ and have the ${0}$ intersection.  

\medskip\noindent
(3)
{\bf Extra credit (40 pts)\/}.
Assume that $E$ is finite-dimensional, and
let $\mapdef{f_i}{E}{E}$ be any 
$p\geq 2$ linear maps such that
\[
f_1 + \cdots + f_p  = \id_E.
\]
Prove that the following properties are equivalent:
\begin{enumerate}
\item[(1)]
$f_i^2 = f_i$, $1\leq i \leq p$.
\item[(2)]
$f_j \circ f_i =  0$, for all $i \not= j$, $1\leq i, j \leq p$.
\end{enumerate}

(1) $\Rightarrow$ (2) 

We multiply $f_i$ on each side of the equation, then get 
\[
f_i^2 + \sum_{j \neq i} f_j \circ f_i = f_i
\]
\[
 \sum_{j \neq i} f_j \circ f_i  = 0
\] \\

Because $E$ is finite-dimensional, then for all $y \in E$ we can write $y = \sum \lambda_i x_i$, where $(x_i)$ is the base. Then for all $y \in E$, we have $ \sum_{j \neq i} f_j \circ f_i (y) = 0$, suppose there is a $k$ which makes $ f_k \circ f_i \neq 0 $ then $ker( f_k \circ f_i ) = E$, thus $ f_k \circ f_i  = 0$, which is a contradiction. \\

(2) $\Rightarrow$ (1)

We multiply $f_i, \; \forall i = 1,2,\cdots, p$ on each side of the equation, then get 
\[
f_i^2 + \sum_{j \neq i} f_j \circ f_i = f_i
\]
\[
 f_i^2 = f_i
\]


\vspace{0.5cm}\noindent
{\bf TOTAL: 200  + 70 points.}

\end{document}
